{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44fdeca5",
   "metadata": {},
   "source": [
    "## USER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8aba69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Parameters Loaded!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "# SCENARIO\n",
    "############################\n",
    "\n",
    "selected_scenario = \"Rare-class Customer Identification\" # Options \"Custom\" , # \"Age-based Spending Patterns\", \"Rare-class Customer Identification\", \"Seasonal Product Preferences\"\n",
    "\n",
    "############################\n",
    "# CUSTOM Parameters Section \n",
    "### Note: selected_scenario = \"Custom\" (above)\n",
    "### Note: Run EITHER Forecasting or Classification (whichever where _columns is not \"None\")\n",
    "############################\n",
    "\n",
    "# Common Parameters\n",
    "noise_level = \"No Noise\"  # Options: No Noise, Low, Medium, High\n",
    "\n",
    "# Forecasting Parameters\n",
    "Forecast_Columns = [\"None\"] # Options: None, Random, Quantity, Net Price, Unit Cost \n",
    "                              # Note: (1) None = No Forecasting (2) multiple values wtih [\"Col1\",\"Col2\"], (3) Brackets are a requirement\n",
    "trend = \"None\"  # Options: None, Linear, Exponential\n",
    "seasonality = \"None\"  # Frequency: None, Weekly, Monthly, Quarterly, Yearly\n",
    "missing_data = \"None\"  # Percentage: None, Low, Medium, High\n",
    "cyclicity = \"None\"  # Options: None, Short Cycles, Long Cycles\n",
    "\n",
    "# Classification & Pattern Parameters (Note: uses Forecasting results above)\n",
    "Classifier_Columns = [\"None\"] # Options: None, Random, Gender, City, State, Country, Continent, Age\n",
    "                            # Note: (1) None = No Forecasting (2) multiple values wtih [\"Col1\",\"Col2\"], (3) Brackets are a requirement\n",
    "class_imbalance = \"No Imbalance\"  # Options: High Imbalance, Medium Imbalance, Low Imbalance, No Imbalance\n",
    "num_classes = 2  # Number of Classes: 2 (Binary), 3, 4, ...\n",
    "class_distribution = \"Even\"  # Distribution: Even, Skewed\n",
    "feature_relations = \"Linearly Separable\"  # Options: Linearly Separable, Non-linearly Separable\n",
    "pattern_type = \"None\"\n",
    "    # None: No specific pattern applied.\n",
    "    # Trend: Introduce a trending pattern.\n",
    "    # Seasonal: Introduce a seasonal pattern.\n",
    "    # Cyclic: Introduce a cyclic pattern.\n",
    "    # Customer Behavior: Simulate specific customer behavior patterns.\n",
    "    # Promotional Impact: Simulate sales spikes due to promotions.\n",
    "    # Inventory Impact: Simulate sales variations due to stock levels.\n",
    "    # Anomalies: Introduce outliers or anomalies.\n",
    "pattern_intensity = \"Mild\" #(if applicable):\n",
    "    # Mild: The pattern is subtle and not very pronounced.\n",
    "    # Moderate: The pattern is clearly visible but not too strong.\n",
    "    # Strong: The pattern is very pronounced.\n",
    "pattern_direction = \"Increasing\" #(for Trend only):\n",
    "    # Increasing: Sales or chosen metric increases over time.\n",
    "    # Decreasing: Sales or chosen metric decreases over time.\n",
    "pattern_frequency = \"Monthly\" #(for Seasonal and Cyclic):\n",
    "    # Weekly: The pattern repeats every week.\n",
    "    # Monthly: The pattern repeats every month.\n",
    "    # Quarterly: The pattern repeats every quarter.\n",
    "    # Yearly: The pattern repeats every year.\n",
    "\n",
    "############################\n",
    "# Othere\n",
    "############################\n",
    "output_format = \"pipe\" #\"csv\" or \"pipe\"\n",
    "\n",
    "\"Parameters Loaded!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49213db5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e79a42da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Loaded'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Load each flat file into a DataFrame\n",
    "#FACTS\n",
    "orders = pd.read_csv('Contoso Files\\Orders.txt', sep='|', encoding='ISO-8859-1')\n",
    "order_rows = pd.read_csv('Contoso Files\\OrderRows.txt', sep='|', encoding='ISO-8859-1')\n",
    "#DIMS\n",
    "geo_locations = pd.read_csv('Contoso Files\\GeoLocations.txt', sep='|', encoding='ISO-8859-1')\n",
    "product = pd.read_csv('Contoso Files\\Product.txt', sep='|', encoding='ISO-8859-1')\n",
    "store = pd.read_csv('Contoso Files\\Store.txt', sep='|', encoding='ISO-8859-1')\n",
    "currency_exchange = pd.read_csv('Contoso Files\\CurrencyExchange.txt', sep='|', encoding='ISO-8859-1')\n",
    "customer = pd.read_csv('Contoso Files\\Customer.txt', sep='|', encoding='ISO-8859-1')\n",
    "date_data = pd.read_csv('Contoso Files\\Date.txt', sep='|', encoding='ISO-8859-1')\n",
    "\n",
    "# Display the first ten rows of each DataFrame\n",
    "dataframes = {\n",
    "    \"GeoLocations\": geo_locations,\n",
    "    \"OrderRows\": order_rows,\n",
    "    \"Orders\": orders,\n",
    "    \"Product\": product,\n",
    "    \"Store\": store,\n",
    "    \"CurrencyExchange\": currency_exchange,\n",
    "    \"Customer\": customer, \n",
    "    \"Date\": date_data\n",
    "}\n",
    "\n",
    "# # Fetching the first 10 rows of each dataframe\n",
    "# first_ten_rows = {name: df.head(10) for name, df in dataframes.items()}\n",
    "# first_ten_rows\n",
    "\n",
    "# UPDATE A SALES SCHEMA\n",
    "sales_df = pd.merge(orders, order_rows, on='OrderKey')\n",
    "sales_df['Order Date'] = pd.to_datetime(sales_df['Order Date'])\n",
    "\n",
    "\"Data Loaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7686d",
   "metadata": {},
   "source": [
    "## SCENARIO FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37d775cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senario Enrichments Loaded'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## SCENARIO 1 - ENRICHMENT 1\n",
    "def S1E1_adjust_spending_by_age(row, adjustments):\n",
    "    \"\"\"Adjust the spending based on the age group.\"\"\"\n",
    "    age = row['Age']\n",
    "    spending = row['Net Price']\n",
    "    \n",
    "    # Define the spending adjustments\n",
    "    if 18 <= age <= 30:\n",
    "        factor = adjustments['18-30']\n",
    "    elif 31 <= age <= 45:\n",
    "        factor = adjustments['31-45']\n",
    "    elif 46 <= age <= 60:\n",
    "        factor = adjustments['46-60']\n",
    "    else:\n",
    "        factor = adjustments['60+']\n",
    "    \n",
    "    # Adjust the spending\n",
    "    return spending * factor\n",
    "\n",
    "## SCENARIO 1 - ENRICHMENT 2: Redefining the adjust_order_time function\n",
    "def S1E2_adjust_order_time(row):\n",
    "    \"\"\"Adjust order time based on age group.\"\"\"\n",
    "    age = row['Age']\n",
    "    order_date = pd.to_datetime(row['Order Date'])\n",
    "    \n",
    "    # Define time-of-day shopping patterns\n",
    "    if 18 <= age <= 30:\n",
    "        hour_adjustment = np.random.choice([20, 21, 22, 23])  # Late night shopping\n",
    "    elif 31 <= age <= 45:\n",
    "        hour_adjustment = np.random.choice([12, 13, 19, 20])  # Mid-day or evening shopping\n",
    "    elif 46 <= age <= 60:\n",
    "        hour_adjustment = np.random.choice([10, 11, 12, 14])  # Morning or early afternoon shopping\n",
    "    else:\n",
    "        hour_adjustment = np.random.choice([9, 10, 11, 15])   # Morning or mid-afternoon shopping\n",
    "    \n",
    "    # Adjust order timestamp\n",
    "    adjusted_order_date = order_date.replace(hour=hour_adjustment)\n",
    "    \n",
    "    # Define day-of-week shopping patterns\n",
    "    if 18 <= age <= 30:\n",
    "        day_adjustment = np.random.choice([4, 5, 6])  # Higher activity during weekends\n",
    "    else:\n",
    "        day_adjustment = np.random.choice(list(range(7)))  # Uniform distribution throughout the week\n",
    "    \n",
    "    # Move the order date to the adjusted day of the week\n",
    "    days_difference = day_adjustment - adjusted_order_date.weekday()\n",
    "    adjusted_order_date += pd.Timedelta(days=days_difference)\n",
    "    \n",
    "    return adjusted_order_date\n",
    "\n",
    "# SCENARIO 1 - ENRICHMENT 3: Enriching the Customer dimension with Preferred Communication Channels based on age\n",
    "def S1E3_assign_communication_channel(age):\n",
    "    \"\"\"Assign preferred communication channel based on age group.\"\"\"\n",
    "    if 18 <= age <= 30:\n",
    "        return \"Email/Mobile Notification\"\n",
    "    elif 31 <= age <= 45:\n",
    "        return \"Email\"\n",
    "    elif 46 <= age <= 60:\n",
    "        return \"Phone Call\"\n",
    "    else:\n",
    "        return \"Direct Mail\"\n",
    "\n",
    "\n",
    "#Scenario 2 - Enrichment 1: Valuable Customers purchase more and more 'premium' products\n",
    "premium_categories = ['Electronics', 'Luxury']\n",
    "\n",
    "def S2E1_adjust_spending_and_preference(row):\n",
    "    adjusted_quantity = row['Quantity']\n",
    "    \n",
    "    # Increase by 20% for valuable customers\n",
    "    if row['ValuableCustomer']:\n",
    "        adjusted_quantity *= 1.2\n",
    "    \n",
    "    # Further increase by 50% for premium products\n",
    "    if row['ValuableCustomer'] and product[product['ProductKey'] == row['ProductKey']]['Category'].values[0] in premium_categories:\n",
    "        adjusted_quantity *= 1.5\n",
    "    \n",
    "    return adjusted_quantity\n",
    "\n",
    "#Scenario 2 - Enrichment 2: Add sales (frequency) for valuable customers\n",
    "def S2E2_generate_additional_orders_for_valuable(valuable_customers, adjusted_sales, num_additional_orders=2):\n",
    "    additional_orders = []\n",
    "    for customer_key in valuable_customers:\n",
    "        customer_orders = adjusted_sales[adjusted_sales['CustomerKey'] == customer_key]\n",
    "        for _ in range(num_additional_orders):\n",
    "            new_order = customer_orders.sample(1).copy()  # Sample an adjusted order\n",
    "            new_order['OrderKey'] = adjusted_sales['OrderKey'].max() + 1  # Assign a new OrderKey\n",
    "            additional_orders.append(new_order)\n",
    "    \n",
    "    return pd.concat(additional_orders)\n",
    "\n",
    "# SCENARIO 3\n",
    "def S3E1_adjust_sales_by_season(row):\n",
    "    month_to_season = {\n",
    "        'December': 'Winter', 'January': 'Winter', 'February': 'Winter',\n",
    "        'March': 'Spring', 'April': 'Spring', 'May': 'Spring',\n",
    "        'June': 'Summer', 'July': 'Summer', 'August': 'Summer',\n",
    "        'September': 'Fall', 'October': 'Fall', 'November': 'Fall'\n",
    "    }\n",
    "    \n",
    "    # Adjusting the adjustment percentage based on product category\n",
    "    adjustment_percentages = {\n",
    "        'Swimwear': 1.4,\n",
    "        'Winter Gear': 1.3,\n",
    "        'Gardening Tools': 1.25,\n",
    "        'School Supplies': 1.2\n",
    "    }\n",
    "    \n",
    "    adjustment = adjustment_percentages.get(row['Category'], 1.1)\n",
    "    \n",
    "    if month_to_season[row['Month']] == row['PeakSeason']:\n",
    "        return row['Net Price'] * adjustment\n",
    "    else:\n",
    "        return row['Net Price']\n",
    "\n",
    "\n",
    "\"Senario Enrichments Loaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ac010",
   "metadata": {},
   "source": [
    "## CUSTOM FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75c1070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_forecasting_features(df, Forecast_Columns, trend, seasonality, missing_data, \n",
    "                                cyclicity, noise_level):\n",
    "    \n",
    "    if Forecast_Columns == [\"None\"]:\n",
    "        return df.copy()\n",
    "    \n",
    "    updated_df = df.copy()\n",
    "    \n",
    "    # Ensure Forecast_Columns is a list\n",
    "    if isinstance(Forecast_Columns, str) or Forecast_Columns == [\"Random\"]:\n",
    "        if Forecast_Columns in (\"Random\", [\"Random\"]):\n",
    "            Forecast_Columns = [np.random.choice(['Quantity', 'Net Price', 'Unit Cost'])]\n",
    "\n",
    "    # Placeholder for adjusted columns\n",
    "    adjusted_data = {col + \"_Adjusted\": updated_df[col].copy() for col in Forecast_Columns}\n",
    "\n",
    "    # Introduce trend\n",
    "    if trend == \"Linear\":\n",
    "        for col in Forecast_Columns:\n",
    "            adjusted_data[col + \"_Adjusted\"] += np.linspace(0, updated_df[col].max() * 0.2, len(updated_df))\n",
    "    elif trend == \"Exponential\":\n",
    "        for col in Forecast_Columns:\n",
    "            adjusted_data[col + \"_Adjusted\"] *= np.linspace(1, 1.2, len(updated_df))\n",
    "    \n",
    "    # Introduce seasonality\n",
    "    if seasonality != \"None\":\n",
    "        freq_map = {\n",
    "            \"Weekly\": 7,\n",
    "            \"Monthly\": 30,\n",
    "            \"Quarterly\": 90,\n",
    "            \"Yearly\": 365\n",
    "        }\n",
    "        seasonal_pattern = np.tile(np.sin(np.linspace(0, 2 * np.pi, freq_map[seasonality])), \n",
    "                                   len(updated_df) // freq_map[seasonality] + 1)[:len(updated_df)]\n",
    "        for col in Forecast_Columns:\n",
    "            adjusted_data[col + \"_Adjusted\"] += updated_df[col].max() * 0.1 * seasonal_pattern\n",
    "    \n",
    "    # Introduce missing data\n",
    "    missing_map = {\n",
    "        \"Low\": 0.05,\n",
    "        \"Medium\": 0.1,\n",
    "        \"High\": 0.2\n",
    "    }\n",
    "    if missing_data in missing_map:\n",
    "        for col in Forecast_Columns:\n",
    "            missing_indices = np.random.choice(updated_df.index, size=int(missing_map[missing_data] * len(updated_df)), \n",
    "                                               replace=False)\n",
    "            adjusted_data[col + \"_Adjusted\"].iloc[missing_indices] = np.nan\n",
    "    \n",
    "    # Introduce cyclicity\n",
    "    if cyclicity == \"Short Cycles\":\n",
    "        cyclical_pattern = np.tile(np.sin(np.linspace(0, 2 * np.pi * 4, len(updated_df))), 1)\n",
    "    elif cyclicity == \"Long Cycles\":\n",
    "        cyclical_pattern = np.tile(np.sin(np.linspace(0, 2 * np.pi, len(updated_df))), 1)\n",
    "    else:\n",
    "        cyclical_pattern = np.ones(len(updated_df))\n",
    "    for col in Forecast_Columns:\n",
    "        adjusted_data[col + \"_Adjusted\"] += updated_df[col].max() * 0.05 * cyclical_pattern\n",
    "    \n",
    "    # Introduce noise\n",
    "    noise_multiplier = {\"Low\": 0.05, \"Medium\": 0.1, \"High\": 0.2}\n",
    "    if noise_level in noise_multiplier:\n",
    "        for col in Forecast_Columns:\n",
    "            noise = np.random.normal(0, updated_df[col].std() * noise_multiplier[noise_level], len(updated_df))\n",
    "            adjusted_data[col + \"_Adjusted\"] += noise\n",
    "\n",
    "    # Add adjusted columns to the original dataframe\n",
    "    for col, data in adjusted_data.items():\n",
    "        updated_df[col] = data\n",
    "\n",
    "    return updated_df\n",
    "\n",
    "def classify_customer_attributes(df, classifier_columns=[\"Random\"], class_imbalance=\"No Imbalance\", \n",
    "                                            num_classes=2, class_distribution=\"Even\", \n",
    "                                            feature_relations=\"Linearly Separable\"):\n",
    "    \"\"\"\n",
    "    Classify customer attributes based on the provided user parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If classifier columns are set to [\"Random\"], select a random column for classification\n",
    "    if classifier_columns == [\"Random\"]:\n",
    "        classifier_columns = [np.random.choice(['Gender', 'City', 'State', 'Country', 'Continent', 'Age'], 1)[0]]\n",
    "    \n",
    "    # Create a combined metric for classification if multiple columns are provided\n",
    "    if len(classifier_columns) > 1:\n",
    "        df['Combined_Metric'] = df[classifier_columns].sum(axis=1)\n",
    "        classifier_column_to_use = 'Combined_Metric'\n",
    "    else:\n",
    "        classifier_column_to_use = classifier_columns[0]\n",
    "    \n",
    "    # Convert categorical columns to integers for classification\n",
    "    if df[classifier_column_to_use].dtype == 'object':\n",
    "        df[classifier_column_to_use] = df[classifier_column_to_use].astype('category').cat.codes\n",
    "\n",
    "    # Applying class imbalance if specified\n",
    "    if class_imbalance != \"No Imbalance\":\n",
    "        # Assuming binary classification for simplicity\n",
    "        if num_classes == 2:\n",
    "            if class_imbalance == \"High Imbalance\":\n",
    "                imbalance_ratio = 0.9\n",
    "            elif class_imbalance == \"Medium Imbalance\":\n",
    "                imbalance_ratio = 0.75\n",
    "            else:  # Low Imbalance\n",
    "                imbalance_ratio = 0.6\n",
    "            df['Class_Label'] = (df[classifier_column_to_use] > df[classifier_column_to_use].quantile(imbalance_ratio)).astype(int)\n",
    "        # For non-binary classification, we'll keep a balanced distribution for simplicity\n",
    "        else:\n",
    "            df['Class_Label'] = pd.qcut(df[classifier_column_to_use], q=num_classes, labels=False, duplicates='drop')\n",
    "    else:\n",
    "        # If no class imbalance is specified, classify based on quantiles\n",
    "        df['Class_Label'] = pd.qcut(df[classifier_column_to_use], q=num_classes, labels=False, duplicates='drop')\n",
    "    \n",
    "    # If feature relations are non-linearly separable, add some noise to the class labels\n",
    "    if feature_relations == \"Non-linearly Separable\":\n",
    "        noise_level = 0.1  # Adding 10% noise for simplicity\n",
    "        num_noisy_samples = int(df.shape[0] * noise_level)\n",
    "        noisy_samples = np.random.choice(df.index, num_noisy_samples, replace=False)\n",
    "        df.loc[noisy_samples, 'Class_Label'] = 1 - df.loc[noisy_samples, 'Class_Label']\n",
    "\n",
    "    return df\n",
    "\n",
    "#####################\n",
    "## PATTERNS\n",
    "#####################\n",
    "\n",
    "def apply_trend_pattern(df, pattern_direction, pattern_intensity):\n",
    "    \"\"\"\n",
    "    Apply a trend pattern to the sales data based on the classification.\n",
    "    \"\"\"\n",
    "    trend_coefficients = {\n",
    "        \"Mild\": 0.05,\n",
    "        \"Moderate\": 0.1,\n",
    "        \"Strong\": 0.2\n",
    "    }\n",
    "    \n",
    "    coeff = trend_coefficients[pattern_intensity]\n",
    "    \n",
    "    # Adjusting the trend coefficient based on the direction\n",
    "    if pattern_direction == \"Decreasing\":\n",
    "        coeff = -coeff\n",
    "        \n",
    "    # Applying the trend pattern based on the class label\n",
    "    for label in df['Class_Label'].unique():\n",
    "        indices = df[df['Class_Label'] == label].index\n",
    "        df.loc[indices, 'Quantity'] = df.loc[indices, 'Quantity'] * (1 + coeff * (label + 1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_seasonal_pattern(df, pattern_frequency, pattern_intensity):\n",
    "    \"\"\"\n",
    "    Apply a seasonal pattern to the sales data based on the classification.\n",
    "    \"\"\"\n",
    "    frequency_map = {\n",
    "        \"Weekly\": 7,\n",
    "        \"Monthly\": 30,\n",
    "        \"Quarterly\": 90,\n",
    "        \"Yearly\": 365\n",
    "    }\n",
    "    \n",
    "    intensity_coefficients = {\n",
    "        \"Mild\": 0.05,\n",
    "        \"Moderate\": 0.1,\n",
    "        \"Strong\": 0.2\n",
    "    }\n",
    "    \n",
    "    coeff = intensity_coefficients[pattern_intensity]\n",
    "    freq = frequency_map[pattern_frequency]\n",
    "    \n",
    "    # Extracting the day of the year to apply the seasonal pattern\n",
    "    day_of_year = df['Order Date'].dt.dayofyear\n",
    "    \n",
    "    # Applying the seasonal pattern based on the class label\n",
    "    for label in df['Class_Label'].unique():\n",
    "        indices = df[df['Class_Label'] == label].index\n",
    "        df.loc[indices, 'Quantity'] = df.loc[indices, 'Quantity'] * (1 + coeff * np.sin(2 * np.pi * day_of_year / freq))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_cyclic_pattern(df, pattern_frequency, pattern_intensity):\n",
    "    \"\"\"\n",
    "    Apply a cyclic pattern to the sales data based on the classification.\n",
    "    \"\"\"\n",
    "    frequency_map = {\n",
    "        \"Weekly\": 52,  # Simulating a cycle that repeats roughly every year\n",
    "        \"Monthly\": 18,  # Simulating a cycle that repeats roughly every 1.5 years\n",
    "        \"Quarterly\": 8,  # Simulating a cycle that repeats roughly every 2 years\n",
    "        \"Yearly\": 4  # Simulating a cycle that repeats roughly every 4 years\n",
    "    }\n",
    "    \n",
    "    intensity_coefficients = {\n",
    "        \"Mild\": 0.05,\n",
    "        \"Moderate\": 0.1,\n",
    "        \"Strong\": 0.2\n",
    "    }\n",
    "    \n",
    "    coeff = intensity_coefficients[pattern_intensity]\n",
    "    freq = frequency_map[pattern_frequency]\n",
    "    \n",
    "    # Extracting the week of the year to apply the cyclic pattern\n",
    "    week_of_year = df['Order Date'].dt.isocalendar().week\n",
    "    \n",
    "    # Applying the cyclic pattern based on the class label\n",
    "    for label in df['Class_Label'].unique():\n",
    "        indices = df[df['Class_Label'] == label].index\n",
    "        df.loc[indices, 'Quantity'] = df.loc[indices, 'Quantity'] * (1 + coeff * np.sin(2 * np.pi * week_of_year / freq))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_customer_behavior_pattern(df, pattern_intensity):\n",
    "    \"\"\"\n",
    "    Apply customer-specific behavior patterns to the sales data based on the classification.\n",
    "    Incorporating both weekend behavior and product preferences.\n",
    "    \"\"\"\n",
    "    intensity_coefficients = {\n",
    "        \"Mild\": 0.05,\n",
    "        \"Moderate\": 0.1,\n",
    "        \"Strong\": 0.2\n",
    "    }\n",
    "    \n",
    "    coeff = intensity_coefficients[pattern_intensity]\n",
    "    \n",
    "    # Extracting the day of the week to apply the customer behavior pattern\n",
    "    day_of_week = df['Order Date'].dt.dayofweek\n",
    "    \n",
    "    # Applying a behavior where customers of certain classes buy more during weekends\n",
    "    for label in df['Class_Label'].unique():\n",
    "        weekend_indices = df[(df['Class_Label'] == label) & (day_of_week >= 5)].index\n",
    "        df.loc[weekend_indices, 'Quantity'] = df.loc[weekend_indices, 'Quantity'] * (1 + coeff)\n",
    "    \n",
    "    # Simulating product preferences for certain customer classes\n",
    "    for label in df['Class_Label'].unique():\n",
    "        preferred_product_indices = df[(df['Class_Label'] == label) & (df['ProductKey'] % 2 == 0)].index\n",
    "        df.loc[preferred_product_indices, 'Quantity'] = df.loc[preferred_product_indices, 'Quantity'] * (1 + coeff)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_promotional_impact_pattern(df, pattern_intensity):\n",
    "    \"\"\"\n",
    "    Apply promotional impact pattern to the sales data based on the classification.\n",
    "    Simulate sales spikes during promotional periods.\n",
    "    \"\"\"\n",
    "    intensity_coefficients = {\n",
    "        \"Mild\": 0.1,\n",
    "        \"Moderate\": 0.2,\n",
    "        \"Strong\": 0.3\n",
    "    }\n",
    "    \n",
    "    coeff = intensity_coefficients[pattern_intensity]\n",
    "    \n",
    "    # Extracting the month to simulate a promotional period\n",
    "    month = df['Order Date'].dt.month\n",
    "    \n",
    "    # Applying a promotional spike for certain months (e.g., holiday sales in December)\n",
    "    promo_indices = df[month == 12].index\n",
    "    df.loc[promo_indices, 'Quantity'] = df.loc[promo_indices, 'Quantity'] * (1 + coeff)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_inventory_impact_pattern(df, pattern_intensity):\n",
    "    \"\"\"\n",
    "    Apply inventory impact pattern to the sales data.\n",
    "    Simulate sales reductions during periods of low stock.\n",
    "    \"\"\"\n",
    "    intensity_coefficients = {\n",
    "        \"Mild\": 0.1,\n",
    "        \"Moderate\": 0.2,\n",
    "        \"Strong\": 0.3\n",
    "    }\n",
    "    \n",
    "    coeff = intensity_coefficients[pattern_intensity]\n",
    "    \n",
    "    # Extracting the month to simulate multiple low inventory periods\n",
    "    month = df['Order Date'].dt.month\n",
    "    \n",
    "    # Applying a sales reduction for specific months (e.g., low stock in February, June, and October)\n",
    "    for m in [2, 6, 10]:\n",
    "        low_stock_indices = df[month == m].index\n",
    "        df.loc[low_stock_indices, 'Quantity'] = df.loc[low_stock_indices, 'Quantity'] * (1 - coeff)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_anomalies_pattern(df, pattern_intensity):\n",
    "    \"\"\"\n",
    "    Apply anomalies pattern to the sales data.\n",
    "    Simulate random spikes or drops in sales quantities.\n",
    "    \"\"\"\n",
    "    intensity_coefficients = {\n",
    "        \"Mild\": 0.1,\n",
    "        \"Moderate\": 0.2,\n",
    "        \"Strong\": 0.3\n",
    "    }\n",
    "    \n",
    "    coeff = intensity_coefficients[pattern_intensity]\n",
    "    \n",
    "    # Randomly select a small percentage of the data to introduce anomalies\n",
    "    anomaly_indices = df.sample(frac=0.02, random_state=42).index\n",
    "    direction = np.random.choice([-1, 1], size=len(anomaly_indices))  # Randomly choose increase or decrease\n",
    "    anomaly_factors = 1 + direction * coeff\n",
    "    \n",
    "    df.loc[anomaly_indices, 'Quantity'] *= anomaly_factors\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc76e0",
   "metadata": {},
   "source": [
    "## FINAL EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d5e06dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been modified based on the 'Rare-class Customer Identification' scenario.\n",
      "Dataframe 'Customer Enriched' exported to Modified Files\\Customer Enriched.txt\n",
      "Dataframe 'Sales Enriched' exported to Modified Files\\Sales Enriched.txt\n"
     ]
    }
   ],
   "source": [
    "def generate_custom_or_scenario_data(selected_scenario, Forecast_Columns, Classifier_Columns, pattern_type, pattern_direction, pattern_intensity, pattern_frequency):\n",
    "    \"\"\"\n",
    "    Generate data based on user's selection of Scenario or Custom parameters.\n",
    "        Parameters:\n",
    "        - selected_scenario: Name of the scenario chosen by the user.\n",
    "        - custom_params: Dictionary containing custom parameters set by the user.\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary containing modified DataFrames based on user's selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    dfs_to_export = {}  # Dictionary to store all dataframes to be exported\n",
    "    \n",
    "    # If the user has chosen a scenario\n",
    "    if selected_scenario != \"Custom\":\n",
    "        if selected_scenario == \"Age-based Spending Patterns\":\n",
    "            # Re-defining the spending adjustments based on the scenario parameters\n",
    "            spending_adjustments = {\n",
    "                '18-30': 1.2,  # 20% increase for High spending\n",
    "                '31-45': 1,   # No change for Medium spending\n",
    "                '46-60': 0.8, # 20% decrease for Low spending\n",
    "                '60+': 1      # No change for Medium spending\n",
    "            }\n",
    "            # Enrichment 1\n",
    "            sales_with_order_details = pd.merge(order_rows, orders[['OrderKey', 'CustomerKey', 'Order Date']], on='OrderKey')\n",
    "            sales_with_age = pd.merge(sales_with_order_details, customer[['CustomerKey', 'Age']], on='CustomerKey')\n",
    "            sales_with_age['Adjusted Net Price'] = sales_with_age.apply(lambda row: S1E1_adjust_spending_by_age(row, spending_adjustments), axis=1)\n",
    "            # Enrichment 2\n",
    "            sales_with_age['Adjusted Order Date'] = sales_with_age.apply(S1E2_adjust_order_time, axis=1)\n",
    "            # Enrichment 3\n",
    "            customer['PreferredCommunication'] = customer['Age'].apply(S1E3_assign_communication_channel)\n",
    "            dfs_to_export['Customer Enriched'] = customer\n",
    "            dfs_to_export['Sales Enriched'] = sales_with_age\n",
    "            print(f\"Data has been modified based on the '{selected_scenario}' scenario.\")\n",
    "        elif selected_scenario == \"Rare-class Customer Identification\": \n",
    "            # Enrichment 1: Labeling Valuable & Unique Spending Patterns\n",
    "            valuable_percentage = 0.05  # 5% of customers\n",
    "            customer['ValuableCustomer'] = False\n",
    "            valuable_customer_indices = np.random.choice(customer.index, int(valuable_percentage * len(customer)), replace=False)\n",
    "            customer.loc[valuable_customer_indices, 'ValuableCustomer'] = True\n",
    "            # Enrichment 2: Higher average order values, Preference for premium products\n",
    "            sales_with_valuable_flag = pd.merge(order_rows, orders[['OrderKey', 'CustomerKey']], on='OrderKey')\n",
    "            sales_with_valuable_flag = pd.merge(sales_with_valuable_flag, customer[['CustomerKey', 'ValuableCustomer']], on='CustomerKey')\n",
    "            sales_with_valuable_flag['Adjusted Quantity'] = sales_with_valuable_flag.apply(S2E1_adjust_spending_and_preference, axis=1)\n",
    "            ## Enrichment 3: frequent shopping\n",
    "            valuable_customers_list = customer[customer['ValuableCustomer']]['CustomerKey'].tolist()\n",
    "            additional_orders = S2E2_generate_additional_orders_for_valuable(valuable_customers_list, sales_with_valuable_flag)\n",
    "            dfs_to_export['Customer Enriched'] = customer\n",
    "            dfs_to_export['Sales Enriched'] = additional_orders\n",
    "            print(f\"Data has been modified based on the '{selected_scenario}' scenario.\")\n",
    "        elif selected_scenario == \"Seasonal Product Preferences\": \n",
    "            seasonal_categories = {\n",
    "                'Swimwear': 'Summer',\n",
    "                'Winter Gear': 'Winter',\n",
    "                'Gardening Tools': 'Spring',\n",
    "                'School Supplies': 'Fall'\n",
    "            }\n",
    "            product['PeakSeason'] = product['Category'].map(seasonal_categories).fillna('None')\n",
    "            sales_with_date = pd.merge(order_rows, orders[['OrderKey', 'Order Date']], on='OrderKey')\n",
    "            sales_with_month = pd.merge(sales_with_date, date_data[['Date', 'Month']], left_on='Order Date', right_on='Date')\n",
    "            sales_with_season = pd.merge(sales_with_month, product[['ProductKey', 'Category', 'PeakSeason']], on='ProductKey')           \n",
    "            sales_with_season['Adjusted Net Price for Season'] = sales_with_season.apply(S3E1_adjust_sales_by_season, axis=1)\n",
    "            dfs_to_export['Sales Enriched'] = sales_with_season[['OrderKey', 'ProductKey', 'Category', 'Net Price', 'Adjusted Net Price for Season', 'Month', 'PeakSeason']]\n",
    "            print(f\"Data has been modified based on the '{selected_scenario}' scenario.\")\n",
    "        else:\n",
    "            print(f\"Scenario '{selected_scenario}' not recognized. No Changes were made\")    \n",
    "    else:\n",
    "        if Forecast_Columns != [\"None\"]:\n",
    "            forecasting_df = add_forecasting_features(sales_df, Forecast_Columns, trend, seasonality, missing_data, cyclicity, noise_level)\n",
    "            dfs_to_export['Sales Enriched'] = forecasting_df\n",
    "            print(\"Data has been modified based on forecasting parameters.\")\n",
    "        if Classifier_Columns != [\"None\"]:\n",
    "            classified_customer_df = classify_customer_attributes(customer.copy())\n",
    "            dfs_to_export['Customer Enriched'] = classified_customer_df\n",
    "            sales_df_classified = pd.merge(sales_df, classified_customer_df[['CustomerKey', 'Class_Label']], on='CustomerKey', how='left')\n",
    "            if pattern_type == \"None\":\n",
    "                dfs_to_export['Sales Enriched'] = sales_df_classified\n",
    "                print(\"Data has been modified based on classification parameters.\")\n",
    "                return dfs_to_export\n",
    "            # Apply trend pattern\n",
    "            sales_df_trend = apply_trend_pattern(sales_df_classified.copy(), pattern_direction, pattern_intensity)\n",
    "            if pattern_type == \"Trend\":\n",
    "                dfs_to_export['Sales Enriched'] = sales_df_trend\n",
    "                print(\"Data has been modified based on classification parameters, Trend pattern.\")\n",
    "                return dfs_to_export\n",
    "            # Apply seasonal pattern\n",
    "            sales_df_seasonal = apply_seasonal_pattern(sales_df_trend.copy(), pattern_frequency, pattern_intensity)\n",
    "            if pattern_type == \"Seasonal\":\n",
    "                dfs_to_export['Sales Enriched'] = sales_df_seasonal\n",
    "                print(\"Data has been modified based on classification parameters, Seasonal pattern.\")\n",
    "                return dfs_to_export\n",
    "            # Apply cyclic pattern\n",
    "            sales_df_cyclic = apply_cyclic_pattern(sales_df_seasonal.copy(), pattern_frequency, pattern_intensity)\n",
    "            if pattern_type == \"Cyclic\":\n",
    "                dfs_to_export['Sales Enriched'] = sales_df_cyclic\n",
    "                print(\"Data has been modified based on classification parameters, Cyclic pattern.\")\n",
    "                return dfs_to_export\n",
    "            # Apply customer behavior pattern\n",
    "            sales_df_behavior_updated = apply_customer_behavior_pattern(sales_df_cyclic.copy(), pattern_intensity)\n",
    "            if pattern_type == \"Customer Behavior\":\n",
    "                dfs_to_export['Sales Enriched'] = sales_df_behavior_updated\n",
    "                print(\"Data has been modified based on classification parameters, CB pattern.\")\n",
    "                return dfs_to_export\n",
    "            # Apply promotional impact pattern\n",
    "            sales_df_promo = apply_promotional_impact_pattern(sales_df_behavior_updated.copy(), pattern_intensity)\n",
    "            if pattern_type == \"Promotional Impact\":\n",
    "                dfs_to_export['Sales Enriched'] = sales_df_promo\n",
    "                print(\"Data has been modified based on classification parameters, PI pattern.\")\n",
    "                return dfs_to_export\n",
    "            # Apply inventory impact pattern\n",
    "            sales_df_inventory_updated = apply_inventory_impact_pattern(sales_df_promo.copy(), pattern_intensity)\n",
    "            if pattern_type == \"Inventory Impact\":\n",
    "                dfs_to_export['Sales Enriched'] = sales_df_inventory_updated\n",
    "                print(\"Data has been modified based on classification parameters, II pattern.\")\n",
    "                return dfs_to_export\n",
    "            # Apply anomalies pattern\n",
    "            sales_df_anomalies = apply_anomalies_pattern(sales_df_inventory_updated.copy(), pattern_intensity)\n",
    "            if pattern_type == \"Anomalies\":\n",
    "                dfs_to_export['Sales Enriched'] = sales_df_anomalies\n",
    "                print(\"Data has been modified based on classification parameters, Anomalies pattern.\")\n",
    "                return dfs_to_export\n",
    "        else: \n",
    "            print(\"No operations selected. Exiting...\")\n",
    "            return None\n",
    "    return dfs_to_export\n",
    "\n",
    "\n",
    "def export_dataframes(dfs, save_directory, format=\"csv\"):\n",
    "    \"\"\"\n",
    "    Export a dictionary of dataframes to CSV or pipe-delimited text files.\n",
    "    \n",
    "    Parameters:\n",
    "    - dfs: Dictionary containing dataframes to be exported.\n",
    "    - save_directory: Directory where files should be saved.\n",
    "    - format: Desired format for export (\"csv\" or \"pipe\").\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for name, df in dfs.items():\n",
    "        if format == \"csv\":\n",
    "            filepath = os.path.join(save_directory, f\"{name}.csv\")\n",
    "            df.to_csv(filepath, index=False)\n",
    "        elif format == \"pipe\":\n",
    "            filepath = os.path.join(save_directory, f\"{name}.txt\")\n",
    "            df.to_csv(filepath, sep=\"|\", index=False)\n",
    "        else:\n",
    "            print(f\"Format '{format}' not recognized.\")\n",
    "            return\n",
    "        print(f\"Dataframe '{name}' exported to {filepath}\")\n",
    "\n",
    "################ EXPORTING ########################\n",
    "dfs_to_export = generate_custom_or_scenario_data(selected_scenario, Forecast_Columns, Classifier_Columns, pattern_type, pattern_direction, pattern_intensity, pattern_frequency)\n",
    "\n",
    "if dfs_to_export:  # Check if dfs_to_export is not None\n",
    "    save_directory = 'Modified Files'\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    export_dataframes(dfs_to_export, save_directory, format=output_format)  # Format can be \"csv\" or \"pipe\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
